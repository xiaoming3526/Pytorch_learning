# Pytorch_learning

## 【Task6(2天)】PyTorch理解更多神经网络优化方法

1. 了解不同优化器
2. 书写优化器代码
3. Momentum
4. 二维优化，随机梯度下降法进行优化实现
5. Ada自适应梯度调节法
6. RMSProp
7. Adam
8. PyTorch种优化器选择


### 1. 了解不同优化器
### 3. Momentum
### 5. Ada自适应梯度调节法
### 6. RMSProp
### 7. Adam
SGD:随机梯度下降法是梯度下降法的一个小变形，就是每次使用一批(batch) 数掘进行梯度的计算，而不是计算全部数据的梯度.因为现在深度学习的数据量都特别大， 所以每次都计算所有数据的梯度是不现实的，这样会导致运算时间特别长，同时每次都计算全部的梯度还失去了一些随机性， 容易陷入局部误差，所以使用随机梯度下降法可能每次都不是朝着真正最小的方向.但是这样反而容易跳出局部极小点。  

Momentum:这种优化方法是在随机梯度下降的同时，增加动量(Momentum) 。这来自于物理中的概念， 可以想象损失函数是一个山谷，一个球从山谷滑下来，在一个平坦的地势，球的滑动速度就会慢下来，可能陷入一些鞍点或者局部极小值点。这个时候给它增加动量就可以让它从高处滑落时的势能转换为平地的功能，相当于惯性增加f小球在平地滑动的速度，从而帮助其跳出鞍点或者局部极小点。动量的计算基于前面梯度，也就是说参数更新不仅仅基于当前的梯度，也基于之前的梯度。

通俗地说，梯度下降的方向是由这一点的方向导数决定的，如果我们考虑小球的惯性，除了这个点上的梯度会决定下一次的前进方向外，上一次前进方向由于惯性也会影响下一次的前进方向，这样两个方向的合力生成下一次小球的加速度。

Ada自适应梯度调节法:
Adagrad：该算法的特点是自动调整学习率，适用于稀疏数据。梯度下降法在每一步对每一个参数使用相同的学习率，这种一刀切的做法不能有效的利用每一个数据集自身的特点。
Adadelta(Adagrad的改进算法)：Adagrad的一个问题在于随着训练的进行，学习率快速单调衰减。Adadelta则使用梯度平方的移动平均来取代全部历史平方和。

RMSProp：RMSprop也是一种学习率调整的算法。Adagrad会累加之前所有的梯度平方，而RMSprop仅仅是计算对应的平均值，因此可缓解Adagrad算法学习率下降较快的问题。

Adam：如果把Adadelta里面梯度的平方和看成是梯度的二阶矩，那么梯度本身的求和就是一阶矩。Adam算法在Adadelta的二次矩基础之上又引入了一阶矩。而一阶矩，其实就类似于动量法里面的动量。


### 2. 书写优化器代码
### 4. 二维优化，随机梯度下降法进行优化实现

部分优化器代码
```python
optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9) #使用SGD 学习率为0.01  momentum为0.9

torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)

```

优化代码实现：https://github.com/xiaoming3526/Pytorch_learning/blob/master/src/Task6/1.py


### 8. PyTorch种优化器选择
在backprop更新权重时，一般采用SGD(Stochastic Gradient Descent)作为优化器。

Adagrad与Adadelta主要解决学习率更新的问题

目前为止 Adam 可能是几种算法中综合表现最好的