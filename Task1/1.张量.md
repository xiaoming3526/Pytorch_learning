PyTorch 基础 : 张量


```
# 首先要引入相关的包
import torch
import numpy as np
torch.__version__

Out[3]: '1.0.0'
```

## 张量(Tensor)
张量的英文是Tensor，它是PyTorch里面基础的运算单位,与Numpy的ndarray相同都表示的是一个多维的矩阵。 与ndarray的最大区别就是，PyTorch的Tensor可以在 GPU 上运行，而 numpy 的 ndarray 只能在 CPU 上运行，在GPU上运行大大加快了运算速度。  
下面我们生成一个简单的张量
```
x = torch.rand(2,3)
x

Out[5]: 
tensor([[0.9805, 0.5673, 0.0320],
        [0.2663, 0.5120, 0.0656]])
```

以上生成了一个，2行3列的的矩阵，我们看一下他的大小：
```
# 可以使用与numpy相同的shape属性查看
print(x.shape)
# 也可以使用size()函数，返回的结果都是相同的
print(x.size())
```
torch.Size([2, 3])  
torch.Size([2, 3])  

下面我们来生成一些多维的张量：  
```
y=torch.rand(2,3,4,5)
print(y.size())
y
```
torch.Size([2, 3, 4, 5])
```
Out[13]: 
#四维空间
tensor([[[[0.6604, 0.0498, 0.2936, 0.7037, 0.8472],
          [0.8354, 0.0164, 0.8999, 0.0742, 0.9962],
          [0.3234, 0.6022, 0.9885, 0.5764, 0.4224],
          [0.8301, 0.8132, 0.2409, 0.3298, 0.3191]],

         [[0.1887, 0.9642, 0.3102, 0.2337, 0.6555],
          [0.3889, 0.7228, 0.2357, 0.4088, 0.2163],
          [0.2862, 0.6695, 0.1998, 0.3705, 0.2343],
          [0.5392, 0.4714, 0.9795, 0.1664, 0.9471]],

         [[0.6596, 0.6455, 0.1298, 0.6386, 0.3054],
          [0.4441, 0.9682, 0.2159, 0.7076, 0.7058],
          [0.6767, 0.0915, 0.8773, 0.6385, 0.8315],
          [0.6749, 0.5995, 0.9705, 0.2631, 0.0546]]],


        [[[0.7691, 0.6872, 0.8742, 0.8098, 0.4407],
          [0.3224, 0.2599, 0.4689, 0.5294, 0.1354],
          [0.4241, 0.4112, 0.5831, 0.6570, 0.0271],
          [0.8413, 0.3332, 0.7290, 0.0741, 0.0423]],

         [[0.9518, 0.2354, 0.3197, 0.6255, 0.5384],
          [0.5749, 0.2463, 0.9858, 0.6100, 0.9727],
          [0.7406, 0.7409, 0.9389, 0.0491, 0.2752],
          [0.1287, 0.2825, 0.4683, 0.2742, 0.2420]],

         [[0.6554, 0.8270, 0.4806, 0.1953, 0.2939],
          [0.0829, 0.2134, 0.0997, 0.4287, 0.7370],
          [0.4220, 0.9908, 0.2073, 0.9416, 0.1460],
          [0.5863, 0.4403, 0.1223, 0.2371, 0.5794]]]])
```
在同构的意义下，第零阶张量 （r = 0） 为**标量 （Scalar）**，第一阶张量 （r = 1） 为**向量 （Vector）**， 第二阶张量 （r = 2） 则成为**矩阵 （Matrix）**，第三阶以上的统称为**多维张量**。  
其中要特别注意的就是标量，我们先生成一个标量：  
```
#我们直接使用现有数字生成
scalar =torch.tensor(3.1433223)
print(scalar)
#打印标量的大小
scalar.size()

tensor(3.1433)
Out[16]:
torch.Size([])
```
对于标量，我们可以直接使用 .item() 从中取出其对应的python对象的数值
```
scalar.item()

Out[17]:
3.143322229385376
```

特别的：如果张量中只有一个元素的tensor也可以调用tensor.item方法
```
tensor = torch.tensor([3.1433223]) 
print(tensor)
tensor.size()

tensor([3.1433])
Out[7]:
torch.Size([1])

tensor.item()

Out[8]:
3.143322229385376
```

## 基本类型
- 32位浮点型：torch.FloatTensor。 (默认)
- 64位整型：torch.LongTensor。
- 32位整型：torch.IntTensor。
- 16位整型：torch.ShortTensor。
- 64位浮点型：torch.DoubleTensor。

除以上数字类型外，还有 byte和chart型  
代码略~~~
```
tensor.long()
tensor.half()
tensor.int()
tensor.float()
tensor.short()
tensor.char()
tensor.byte()
```

## Numpy转换
```
# 使用numpy方法将Tensor转为ndarray
a = torch.randn((3, 2))
# tensor转化为numpy
numpy_a = a.numpy()
print(numpy_a)

# numpy转化为Tensor
torch_a = torch.from_numpy(numpy_a)
torch_a
```
Tensor和numpy对象共享内存，所以他们之间的转换很快，而且几乎不会消耗什么资源。但这也意味着，如果其中一个变了，另外一个也会随之改变。

## 设备间转换
般情况下可以使用.cuda方法将tensor移动到gpu，这步操作需要cuda设备支持  
ps:自己没安装cude版本 就没进行测试

## 初始化
Pytorch中有许多默认的初始化方法可以使用 比较简单就不进行展示了~
```
# 使用[0,1]均匀分布随机初始化二维数组
rnd = torch.rand(5, 3)
rnd

##初始化，使用1填充
one = torch.ones(2, 2)
one


##初始化，使用0填充
zero=torch.zeros(2,2)
zero

#初始化一个单位矩阵，即对角线为1 其他为0
eye=torch.eye(2,2)
eye
```
## 常用方法
PyTorch中对张量的操作api 和 NumPy 非常相似，如果熟悉 NumPy 中的操作，那么 他们二者 基本是一致的：
```
x = torch.randn(3, 3)
print(x)

tensor([[0.2357, 0.9269, 0.9339],
        [0.4123, 0.6061, 0.0805],
        [0.7333, 0.6702, 0.6833]])

# 沿着行取最大值
max_value, max_idx = torch.max(x, dim=1)
print(max_value, max_idx)

tensor([0.9339, 0.6061, 0.7333]) tensor([2, 1, 0])

#每行x求和
torch.sum(x, dim=1)
Out[37]: tensor([2.0965, 1.0988, 2.0868])
```
```
y=torch.randn(3, 3)
z = x + y
print(z)

Out[42]: 
tensor([[-1.7363,  0.2795,  0.5696],
        [-1.0374, -1.8606,  1.6849],
        [-0.2148,  0.5633, -0.0276]])

#正如官方60分钟教程中所说，以_为结尾的，均会改变调用值
# add 完成后x的值改变了
x.add_(y)
print(x)

Out[43]: 
tensor([[-1.7363,  0.2795,  0.5696],
        [-1.0374, -1.8606,  1.6849],
        [-0.2148,  0.5633, -0.0276]])
```