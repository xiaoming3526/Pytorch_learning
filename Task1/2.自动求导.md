# PyTorch 基础 : 自动求导

深度学习的算法本质上是通过反向传播求导数，而PyTorch的autograd模块则实现了此功能。在Tensor上的所有操作，autograd都能为它们自动提供微分，避免了手动计算导数的复杂过程。

从0.4版本起, Variable 正式合并入Tensor, Variable 本来实现的自动微分功能，Tensor就能支持。读者还是可以使用Variable(tensor), 但是这个操作其实什么都没做。

所以，以后的代码建议直接使用Tensor，因为官方文档中已经将Variable设置成过期模块

要想使得Tensor使用autograd功能，只需要设置tensor.requries_grad=True

```
import torch
torch.__version__

Out[3]: '1.0.0'
```

在张量创建时，通过设置 requires_grad 标识为Ture来告诉Pytorch需要对该张量进行自动求导，PyTorch会记录该张量的每一步操作历史并自动计算

```
x = torch.rand(5, 5, requires_grad=True)
x

Out[5]: 
tensor([[0.4179, 0.2548, 0.2601, 0.5463, 0.5750],
        [0.5537, 0.4285, 0.6197, 0.4930, 0.2800],
        [0.6038, 0.6700, 0.0126, 0.6806, 0.8002],
        [0.1011, 0.8438, 0.2780, 0.7889, 0.6524],
        [0.4778, 0.8054, 0.7468, 0.1591, 0.2123]], requires_grad=True)

y = torch.rand(5,5,requires_grad=True)
y

Out[7]: 
tensor([[0.3751, 0.0948, 0.0397, 0.9976, 0.7338],
        [0.1136, 0.8607, 0.6554, 0.6136, 0.0365],
        [0.2242, 0.4609, 0.4456, 0.3037, 0.9190],
        [0.4627, 0.2415, 0.2297, 0.1595, 0.9584],
        [0.5481, 0.5649, 0.6767, 0.6799, 0.1376]], requires_grad=True)
```
我们看到 该张量的grad_fn已经被赋予了一个新的函数。下面我们来调用反向传播函数，计算其梯度
```
z = torch.sum(x+y)
z

Out[9]: tensor(23.7948, grad_fn=<SumBackward0>)
```

## 简单的自动求导
```
z.backward()
print(x.grad,y.grad)

#为啥为1那 因为正向传播是加法
tensor([[1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.]]) 
tensor([[1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.]])
```

## 复杂的自动求导

```
x = torch.rand(5, 5, requires_grad=True)
y = torch.rand(5, 5, requires_grad=True)
z= x**2+y**3
z

Out[16]: 
tensor([[0.1944, 0.9136, 0.9056, 1.1472, 0.5293],
        [0.3953, 0.0336, 0.3129, 0.6609, 0.3175],
        [1.0480, 0.6250, 1.0129, 1.0602, 0.6970],
        [0.2683, 0.0750, 0.7514, 0.1686, 0.8023],
        [0.8138, 1.1733, 0.2708, 0.2458, 0.9375]], 


#我们的返回值不是一个scalar，所以需要输入一个大小相同的张量作为参数，这里我们用ones_like函数根据x生成一个张量

z.backward(torch.ones_like(x))
print(x.grad)

tensor([[0.1387, 0.5409, 1.5078, 1.9187, 1.3080],
        [1.2506, 0.3644, 0.6322, 0.2100, 0.3453],
        [1.8567, 1.5535, 1.4846, 0.5618, 0.2480],
        [0.8102, 0.4617, 0.5741, 0.7437, 1.7050],
        [1.6568, 1.8448, 0.8775, 0.5659, 1.2392]])
```
我们可以使用with torch.no_grad()上下文管理器临时禁止对已设置requires_grad=True的张量进行自动求导。这个方法在测试集计算准确率的时候会经常用到，例如
```
with torch.no_grad():
    print((x +y*2).requires_grad)

False
```